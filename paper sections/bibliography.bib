
@article{dusseldorp_combining_2010,
	title = {Combining an additive and tree-based regression model simultaneously: {STIMA}},
	volume = {19},
	issn = {1061-8600},
	shorttitle = {Combining an {Additive} and {Tree}-{Based} {Regression} {Model} {Simultaneously}},
	doi = {10.1198/jcgs.2010.06089},
	abstract = {Additive models and tree-based regression models are two main classes of statistical models used to predict the scores on a continuous response variable. It is known

that additive models become very complex in the presence of higher order interaction

effects, whereas some tree-based models, such as CART, have problems capturing linear main effects of continuous predictors. To overcome these drawbacks, the regression

trunk model has been proposed: a multiple regression model with main effects and a

parsimonious amount of higher order interaction effects. The interaction effects can be

represented by a small tree: a regression trunk. This article proposes a new algorithm{\textemdash}

Simultaneous Threshold Interaction Modeling Algorithm (STIMA){\textemdash}to estimate a regression trunk model that is more general and more efficient than the initial one (RTA)

and is implemented in the R-package stima. Results from a simulation study show

that the performance of STIMA is satisfactory for sample sizes of 200 or higher. For

sample sizes of 300 or higher, the 0.50 SE rule is the best pruning rule for a regression

trunk in terms of power and Type I error. For sample sizes of 200, the 0.80 SE rule is

recommended. Results from a comparative study of eight regression methods applied

to ten benchmark datasets suggest that STIMA and GUIDE are the best performers

in terms of cross-validated prediction error. STIMA appeared to be the best method for

datasets containing many categorical variables. The characteristics of a regression trunk

model are illustrated using the Boston house price dataset.

Supplemental materials for this article, including the R-package stima, are available online.},
	number = {3},
	urldate = {2010-08-20},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Dusseldorp, Elise},
	month = aug,
	year = {2010},
	keywords = {CART, generalized additive modeling, interactions, project: hierarchical additive model tutorial paper, regression, unread},
	pages = {514--530},
	file = {American Statistical Association - Journal of Computational and Graphical Statistics - 0(0)\:1:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\RUBUMTPD\\jcgs.2010.html:text/html}
}

@article{abel_nonparametric_2002,
	title = {Nonparametric modeling and spatiotemporal dynamical systems},
	abstract = {In this article, it is described how to use statistical data analysis to obtain models directly from data. The focus is put on finding nonlinearities within a generalized additive model. These models are found by the means of backfitting algorithms or more general versions, like the alternating conditional expectation value method. The method is illustrated by numerically generated data. As an application the example of vortex ripple dynamics, a highly complex fluid-granular system is treated.},
	urldate = {2009-11-02},
	journal = {Arxiv},
	author = {Abel, M.},
	month = feb,
	year = {2002},
	keywords = {generalized additive modeling, nonparametric estimation, project: hierarchical additive model tutorial paper, spatial dynamics, time series and statistics for dynamic systems, unread},
	file = {arXiv.org Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\4VT5HKC4\\0202058.html:text/html}
}

@article{wood_partially_2001,
	title = {Partially specified ecological models},
	volume = {71},
	issn = {0012-9615},
	doi = {10.1890/0012-9615(2001)071[0001:PSEM]2.0.CO;2},
	abstract = {Models are useful when they are compared with data. Whether this comparison should be qualitative or quantitative depends on circumstances, but in many cases some statistical comparison of model and data is useful and enhances objectivity. Unfortunately, ecological dynamic models tend to contain assumptions and simplifications that enhance tractability, promote insight, but spoil model fit, and this can cause difficulties when adopting a statistical approach. Furthermore, the arcane numerical analysis required to fit dynamic models reliably presents an impediment to objective model testing by fitting. This paper presents methods for formulating and fitting partially specified models, which aim to achieve a measure of generality by avoiding some of the irrelevant incidental assumptions that are inevitable in more traditional approaches. This is done by allowing delay differential equation models, difference equation models, and differential equation models to be constructed with part of their structure represented by unknown functions, while part of the structure may contain conventional model elements that contain only unknown parameters. An integrated practical methodology for using such models is presented along with several examples, which include use of models formulated using delay differential equations, discrete difference equations/matrix models, ordinary differential equations, and partial differential equations. The methods also allow better estimation from ecological data by model fitting, since models can be formulated to include fewer unjustified assumptions than would usually be the case if more traditional models were used, while still including as much structure as the modeler believes can be justified by biological knowledge: model structure improves precision, while fewer extraneous assumptions reduce unquantifiable bias.},
	number = {1},
	urldate = {2010-06-14},
	journal = {Ecological Monographs},
	author = {Wood, Simon N.},
	month = feb,
	year = {2001},
	keywords = {dynamics, generalized additive modeling, important reference, key reference, mechanistic modeling, project: hierarchical additive model tutorial paper, Project: nonparametric functional form paper, read, semi-mechanistic modeling, semiparametric estimation, statistics},
	pages = {1--25},
	file = {ESA Online Journals - PARTIALLY SPECIFIED ECOLOGICAL MODELS:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\9F2I9D9H\\0012-9615(2001)071[0001PSEM]2.0.html:text/html}
}

@article{hazelton_semiparametric_2011,
	title = {Semiparametric regression with shape-constrained penalized splines},
	volume = {55},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2011.04.018},
	abstract = {In semiparametric regression models, penalized splines can be used to describe complex, non-linear relationships between the mean response and covariates. In some applications it is desirable to restrict the shape of the splines so as to enforce properties such as monotonicity or convexity on regression functions. We describe a method for imposing such shape constraints on penalized splines within a linear mixed model framework. We employ Markov chain Monte Carlo (MCMC) methods for model fitting, using a truncated prior distribution to impose the requisite shape restrictions. We develop a computationally efficient MCMC sampler by using a correspondingly truncated multivariate normal proposal distribution, which is a restricted version of the approximate sampling distribution of the model parameters in an unconstrained version of the model. We also describe a cheap approximation to this methodology that can be applied for shape-constrained scatterplot smoothing. Our methods are illustrated through two applications, the first involving the length of dugongs and the second concerned with growth curves for sitka spruce trees.},
	number = {10},
	urldate = {2012-12-04},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hazelton, Martin L. and Turlach, Berwin A.},
	month = oct,
	year = {2011},
	keywords = {bayesian statistics, frequentist statistics, functional mixed effect models, generalized additive modeling, indexed, markov chain Monte Carlo, mixed models, penalization, project: hierarchical additive model tutorial paper, regression, Shape constraint, spline regression},
	pages = {2871--2879}
}

@article{sangalli_spatial_2013,
	title = {Spatial spline regression models},
	copyright = {{\textcopyright} 2013 Royal Statistical Society},
	issn = {1467-9868},
	doi = {10.1111/rssb.12009},
	abstract = {We describe a model for the analysis of data distributed over irregularly shaped spatial domains with complex boundaries, strong concavities and interior holes. Adopting an approach that is typical of functional data analysis, we propose a spatial spline regression model that is computationally efficient, allows for spatially distributed covariate information and can impose various conditions over the boundaries of the domain. Accurate surface estimation is achieved by the use of piecewise linear and quadratic finite elements.},
	language = {en},
	urldate = {2013-04-02},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Sangalli, Laura M. and Ramsay, James O. and Ramsay, Timothy O.},
	year = {2013},
	keywords = {boundary effects, generalized additive modeling, nonparametric estimation, penalization, project: hierarchical additive model tutorial paper, read, regression, smoothing, spatial statistics, spline regression, splines, statistics, statistics for dependence},
	pages = {n/a--n/a},
	file = {Sangalli_2013_Spatial spline regression models.pdf:D\:\\Documents\\papers\\indexed_papers\\Sangalli_2013_Spatial spline regression models.pdf:application/pdf;Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\Z76DSBVZ\\abstract.html:text/html}
}

@article{chu_search_2008,
	title = {Search for additive nonlinear time series causal models},
	volume = {9},
	issn = {1532-4435},
	abstract = {Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efficiently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model specification procedure is able to extract relatively detailed causal information. We investigate the finite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed.},
	urldate = {2013-04-18},
	journal = {J. Mach. Learn. Res.},
	author = {Chu, Tianjiao and Glymour, Clark},
	month = jun,
	year = {2008},
	keywords = {causality, frequentist statistics, generalized additive modeling, important reference, nonparametric estimation, parameter estimation, project: forecasting marine species dynamics, project: hierarchical additive model tutorial paper, read, spline regression, splines, statistics, structural equation modeling, time series and statistics for dynamic systems, unread},
	pages = {967--991},
	file = {ACM Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\8CRICFZX\\citation.html:text/html;Chu_2008_Search for additive nonlinear time series causal models.pdf:D\:\\Documents\\papers\\indexed_papers\\Chu_2008_Search for additive nonlinear time series causal models.pdf:application/pdf}
}

@article{heckman_penalized_2013,
	title = {Penalized regression, mixed effects models and appropriate modelling},
	volume = {7},
	issn = {1935-7524},
	doi = {10.1214/13-EJS809},
	abstract = {Linear mixed effects methods for the analysis of longitudinal data provide a convenient framework for modelling within-individual correlation across time. Using spline functions allows for flexible modelling of the response as a smooth function of time. A computational connection between linear mixed effects modelling and spline smoothing has resulted in a cross-fertilization of these two fields. The connection has popularized the use of spline functions in longitudinal data analysis and the use of mixed effects software in smoothing analyses. However, care must be taken in exploiting this connection, as resulting estimates of the underlying population mean might not track the data well and associated standard errors might not reflect the true variability in the data. We discuss these shortcomings and suggest some easy-to-compute methods to eliminate them.},
	language = {EN},
	urldate = {2013-06-04},
	journal = {Electronic Journal of Statistics},
	author = {Heckman, Nancy and Lockhart, Richard and Nielsen, Jason D.},
	year = {2013},
	keywords = {bayesian statistics, functional mixed effect models, generalized additive modeling, mixed models, penalization, project: hierarchical additive model tutorial paper, smoothing, unread},
	pages = {1517--1552},
	file = {Heckman_2013_Penalized regression, mixed effects models and appropriate modelling.pdf:D\:\\Documents\\papers\\indexed_papers\\Heckman_2013_Penalized regression, mixed effects models and appropriate modelling.pdf:application/pdf}
}

@article{wood_soap_2008,
	title = {Soap film smoothing},
	volume = {70},
	copyright = {{\textcopyright} 2008 Royal Statistical Society},
	issn = {1467-9868},
	doi = {10.1111/j.1467-9868.2008.00665.x},
	abstract = {Summary. Conventional smoothing methods sometimes perform badly when used to smooth data over complex domains, by smoothing inappropriately across boundary features, such as peninsulas. Solutions to this smoothing problem tend to be computationally complex, and not to provide model smooth functions which are appropriate for incorporating as components of other models, such as generalized additive models or mixed additive models. We propose a class of smoothers that are appropriate for smoothing over difficult regions of 2 which can be represented in terms of a low rank basis and one or two quadratic penalties. The key features of these smoothers are that they do not {\textquoteleft}smooth across{\textquoteright} boundary features, that their representation in terms of a basis and penalties allows straightforward incorporation as components of generalized additive models, mixed models and other non-standard models, that smoothness selection for these model components is straightforward to accomplish in a computationally efficient manner via generalized cross-validation, Akaike's information criterion or restricted maximum likelihood, for example, and that their low rank means that their use is computationally efficient.},
	language = {en},
	number = {5},
	urldate = {2013-06-21},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N. and Bravington, Mark V. and Hedley, Sharon L.},
	year = {2008},
	keywords = {generalized additive modeling, penalization, project: forecasting marine species dynamics, project: hierarchical additive model tutorial paper, read, regression, smoothing, spatial statistics, statistics},
	pages = {931--955},
	file = {Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\3IQRIWN9\\abstract.html:text/html}
}

@article{miller_finite_2014,
	title = {Finite area smoothing with generalized distance splines},
	issn = {1352-8505, 1573-3009},
	doi = {10.1007/s10651-014-0277-4},
	abstract = {Most conventional spatial smoothers smooth with respect to the Euclidean distance between observations, even though this distance may not be a meaningful measure of spatial proximity, especially when boundary features are present. When domains have complicated boundaries leakage (the inappropriate linking of parts of the domain which are separated by physical barriers) can occur. To overcome this problem, we develop a method of smoothing with respect to generalized distances, such as within domain distances. We obtain the generalized distances between our points and then use multidimensional scaling to find a configuration of our observations in a Euclidean space of 2 or more dimensions, such that the Euclidian distances between points in that space closely approximate the generalized distances between the points. Smoothing is performed over this new point configuration, using a conventional smoother. To mitigate the problems associated with smoothing in high dimensions we use a generalization of thin plate spline smoothers proposed by Duchon (Constructive theory of functions of several variables, pp 85{\textendash}100, 1977). This general method for smoothing with respect to generalized distances improves on the performance of previous within-domain distance spatial smoothers, and often provides a more natural model than the soap film approach of Wood et al. (J R Stat Soc Ser B Stat Methodol 70(5):931{\textendash}955, 2008). The smoothers are of the linear basis with quadratic penalty type easily incorporated into a range of statistical models.},
	language = {en},
	urldate = {2014-03-02},
	journal = {Environmental and Ecological Statistics},
	author = {Miller, David L. and Wood, Simon N.},
	year = {2014},
	keywords = {boundary effects, generalized additive modeling, Multidimensional scaling, multivariate statistics, nonparametric estimation, project: hierarchical additive model tutorial paper, read, smoothing, spatial statistics, statistics},
	pages = {1--17},
	file = {Miller_2014_Finite area smoothing with generalized distance splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Miller_2014_Finite area smoothing with generalized distance splines.pdf:application/pdf}
}

@article{morrissey_search_2014,
	title = {In search of the best methods for multivariate selection analysis},
	copyright = {This article is protected by copyright. All rights reserved.},
	issn = {2041-210X},
	doi = {10.1111/2041-210X.12259},
	abstract = {1.Regression is an important method for characterising the form of natural selection from individual-based data. Many kinds of regression analysis exist, but few are regularly employed in studies of natural selection. I provide an overview of some of the main underused types of regression analysis by applying them all to test analyses of viability selection for lamb traits in Soay sheep (Ovis aries). This exercise highlights known problems with existing methods, uncovers some new ones, and also reveals ways to harness underused methods to get around these problems. 2.I first estimate selection gradients using generalised linear models, combined with recentlypublished methods for obtaining quantitatively interpretable selection gradient estimates from arbitrary regression models of trait-fitness relationships. I then also apply generalised ridge regression, the lasso, and projection-pursuit regression, in each case also deriving selection gradients. I compare inferences of non-linear selection by diagonalisation of the matrix and by projection-pursuit regression. 3.Selection gradient estimates generally correspond across different regression methods. Although there is little evidence for non-linear selection in the test datasets, very problematic aspects of the behaviour of analysis based on diagonalisation of the are apparent. In addition to better-known problems, (i) the direction and magnitude of estimated major axes of quadratic selection are biased toward directions of phenotype that have little variance, and (ii) the magnitudes of selection of major axes of variance-standardised are not themselves interpretable in any standardised way. 4.While all regression-based methods for analysis of selection have useful properties, projectionpursuit regression seems to stand out. This method can: (i) provide both dimensionalityreduction, (ii) be the basis for inference of quantitatively interpretable selection gradients, and (iii) by characterising major axes of selection, rather than of linear or quadratic selection separately, provide biologically-interpretable inference of non-linear selection. This article is protected by copyright. All rights reserved.},
	language = {en},
	urldate = {2014-09-05},
	journal = {Methods in Ecology and Evolution},
	author = {Morrissey, Michael B.},
	month = sep,
	year = {2014},
	keywords = {generalised linear models, generalized additive modeling, model selection, multivariate statistics, penalization, project: hierarchical additive model tutorial paper, regression, regularization, smoothing, statistics for high dimensional systems},
	pages = {n/a--n/a},
	file = {Morrissey_2014_In search of the best methods for multivariate selection analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Morrissey_2014_In search of the best methods for multivariate selection analysis.pdf:application/pdf}
}

@article{papp_shape-constrained_2014,
	title = {Shape-constrained estimation using nonnegative splines},
	volume = {23},
	issn = {1061-8600},
	doi = {10.1080/10618600.2012.707343},
	abstract = {We consider the problem of nonparametric estimation of unknown smooth functions in the presence of restrictions on the shape of the estimator and on its support using polynomial splines. We provide a general computational framework that treats these estimation problems in a unified manner, without the limitations of the existing methods. Applications of our approach include computing optimal spline estimators for regression, density estimation, and arrival rate estimation problems in the presence of various shape constraints. Our approach can also handle multiple simultaneous shape constraints. The approach is based on a characterization of nonnegative polynomials that leads to semidefinite programming (SDP) and second-order cone programming (SOCP) formulations of the problems. These formulations extend and generalize a number of previous approaches in the literature, including those with piecewise linear and B-spline estimators. We also consider a simpler approach in which nonnegative splines are approximated by splines whose pieces are polynomials with nonnegative coefficients in a nonnegative basis. A condition is presented to test whether a given nonnegative basis gives rise to a spline cone that is dense in the space of nonnegative continuous functions. The optimization models formulated in the article are solvable with minimal running time using off-the-shelf software. We provide numerical illustrations for density estimation and regression problems. These examples show that the proposed approach requires minimal computational time, and that the estimators obtained using our approach often match and frequently outperform kernel methods and spline smoothing without shape constraints. Supplementary materials for this article are provided online.},
	number = {1},
	urldate = {2014-12-30},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Papp, D{\'a}vid and Alizadeh, Farid},
	month = jan,
	year = {2014},
	keywords = {density estimation, generalized additive modeling, project: hierarchical additive model tutorial paper, regression, Shape constraint, smoothing, spline regression, splines, statistics, unread},
	pages = {211--231},
	file = {Papp_2014_Shape-Constrained Estimation Using Nonnegative Splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Papp_2014_Shape-Constrained Estimation Using Nonnegative Splines.pdf:application/pdf}
}

@article{guo_functional_2002,
	title = {Functional mixed effects models},
	volume = {58},
	issn = {1541-0420},
	doi = {10.1111/j.0006-341X.2002.00121.x},
	abstract = {Summary.  In this article, a new class of functional models in which smoothing splines are used to model fixed effects as well as random effects is introduced. The linear mixed effects models are extended to non-parametric mixed effects models by introducing functional random effects, which are modeled as realizations of zero-mean stochastic processes. The fixed functional effects and the random functional effects are modeled in the same functional space, which guarantee the population-average and subject-specific curves have the same smoothness property. These models inherit the flexibility of the linear mixed effects models in handling complex designs and correlation structures, can include continuous covariates as well as dummy factors in both the fixed or random design matrices, and include the nested curves models as special cases. Two estimation procedures are proposed. The first estimation procedure exploits the connection between linear mixed effects models and smoothing splines and can be fitted using existing software. The second procedure is a sequential estimation procedure using Kalman filtering. This algorithm avoids inversion of large dimensional matrices and therefore can be applied to large data sets. A generalized maximum likelihood (GML) ratio test is proposed for inference and model selection. An application to comparison of cortisol profiles is used as an illustration.},
	language = {en},
	number = {1},
	urldate = {2015-01-12},
	journal = {Biometrics},
	author = {Guo, Wensheng},
	year = {2002},
	keywords = {functional mixed effect models, functional response, generalized additive modeling, mixed models, multivariate statistics, project: hierarchical additive model tutorial paper, statistics, statistics for functional data, statistics for high dimensional systems, unread},
	pages = {121--128},
	file = {Guo_2002_Functional Mixed Effects Models.pdf:D\:\\Documents\\papers\\indexed_papers\\Guo_2002_Functional Mixed Effects Models.pdf:application/pdf}
}

@article{wood_thin_2003,
	title = {Thin plate regression splines},
	volume = {65},
	number = {1},
	urldate = {2016-01-31},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2003},
	keywords = {generalized additive modeling, project: hierarchical additive model tutorial paper, read, regression, smoothing, spline regression, statistics for high dimensional systems, theoretical methods},
	pages = {95--114},
	file = {Wood_2003_Thin plate regression splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2003_Thin plate regression splines.pdf:application/pdf}
}

@article{baayen_autocorrelated_2016,
	title = {Autocorrelated errors in experimental data in the language sciences: {Some} solutions offered by {Generalized} {Additive} {Mixed} {Models}},
	shorttitle = {Autocorrelated errors in experimental data in the language sciences},
	abstract = {A problem that tends to be ignored in the statistical analysis of experimental data in the language sciences is that responses often constitute time series, which raises the problem of autocorrelated errors. If the errors indeed show autocorrelational structure, evaluation of the significance of predictors in the model becomes problematic due to potential anti-conservatism of p-values. This paper illustrates two tools offered by Generalized Additive Mixed Models (GAMMs) (Lin and Zhang, 1999; Wood, 2006, 2011, 2013) for dealing with autocorrelated errors, as implemented in the current version of the fourth author's mgcv package (1.8.9): the possibility to specify an ar(1) error model for Gaussian models, and the possibility of using factor smooths for random-effect factors such as subject and item. These factor smooths are set up to have the same smoothing parameters, and are penalized to yield the non-linear equivalent of random intercepts and random slopes in the classical linear framework. Three case studies illustrate these issues.},
	urldate = {2016-02-07},
	journal = {arXiv:1601.02043 [stat]},
	author = {Baayen, R. Harald and van Rij, Jacolien and de Cat, Cecile and Wood, Simon N.},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.02043},
	keywords = {auto-correlation, functional mixed effect models, functional regression, generalized additive modeling, mixed models, nonlinear regression, project: hierarchical additive model tutorial paper, skimmed, statistics for high dimensional systems},
	file = {Baayen_2016_Autocorrelated errors in experimental data in the language sciences.pdf:D\:\\Documents\\papers\\indexed_papers\\Baayen_2016_Autocorrelated errors in experimental data in the language sciences.pdf:application/pdf}
}

@article{wood_fast_2011,
	title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	volume = {73},
	copyright = {{\textcopyright} 2010 Royal Statistical Society},
	issn = {1467-9868},
	doi = {10.1111/j.1467-9868.2010.00749.x},
	abstract = {Summary.  Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton{\textendash}Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
	language = {en},
	number = {1},
	urldate = {2016-02-28},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	month = jan,
	year = {2011},
	keywords = {functional mixed effect models, generalized additive modeling, maximum likelihood estimation, mixed models, parameter estimation, project: hierarchical additive model tutorial paper, random processes, restricted maximum likelihood, skimmed, smoothing, statistics for functional data},
	pages = {3--36},
	file = {Wood_2011_Fast stable restricted maximum likelihood and marginal likelihood estimation of.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2011_Fast stable restricted maximum likelihood and marginal likelihood estimation of.pdf:application/pdf}
}

@article{wood_generalized_2015,
	title = {Generalized additive models for large data sets},
	volume = {64},
	copyright = {{\textcopyright} 2014 The Authors. Journal of the Royal Statistical Society: Series C Applied Statistics Published by John Wiley \& Sons Ltd on behalf of the Royal Statistical Society., This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.},
	issn = {1467-9876},
	doi = {10.1111/rssc.12068},
	abstract = {We consider an application in electricity grid load prediction, where generalized additive models are appropriate, but where the data set's size can make their use practically intractable with existing methods. We therefore develop practical generalized additive model fitting methods for large data sets in the case in which the smooth terms in the model are represented by using penalized regression splines. The methods use iterative update schemes to obtain factors of the model matrix while requiring only subblocks of the model matrix to be computed at any one time. We show that efficient smoothing parameter estimation can be carried out in a well-justified manner. The grid load prediction problem requires updates of the model fit, as new data become available, and some means for dealing with residual auto-correlation in grid load. Methods are provided for these problems and parallel implementation is covered. The methods allow estimation of generalized additive models for large data sets by using modest computer hardware, and the grid load prediction problem illustrates the utility of reduced rank spline smoothing methods for dealing with complex modelling problems.},
	language = {en},
	number = {1},
	urldate = {2016-03-02},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
	month = jan,
	year = {2015},
	keywords = {generalized additive modeling, project: hierarchical additive model tutorial paper, regression, skimmed, statistics for functional data, statistics for large datasets},
	pages = {139--155},
	file = {Full Text PDF:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\AUPPM753\\Wood et al. - 2015 - Generalized additive models for large data sets.pdf:application/pdf}
}

@article{wood_straightforward_2012,
	title = {Straightforward intermediate rank tensor product smoothing in mixed models},
	volume = {23},
	issn = {0960-3174, 1573-1375},
	doi = {10.1007/s11222-012-9314-z},
	abstract = {Tensor product smooths provide the natural way of representing smooth interaction terms in regression models because they are invariant to the units in which the covariates are measured, hence avoiding the need for arbitrary decisions about relative scaling of variables. They would also be the natural way to represent smooth interactions in mixed regression models, but for the fact that the tensor product constructions proposed to date are difficult or impossible to estimate using most standard mixed modelling software. This paper proposes a new approach to the construction of tensor product smooths, which allows the smooth to be written as the sum of some fixed effects and some sets of i.i.d. Gaussian random effects: no previously published construction achieves this. Because of the simplicity of this random effects structure, our construction is useable with almost any flexible mixed modelling software, allowing smooth interaction terms to be readily incorporated into any Generalized Linear Mixed Model. To achieve the computationally convenient separation of smoothing penalties, the construction differs from previous tensor product approaches in the penalties used to control smoothness, but the penalties have the advantage over several alternative approaches of being explicitly interpretable in terms of function shape. Like all tensor product smoothing methods, our approach builds up smooth functions of several variables from marginal smooths of lower dimension, but unlike much of the previous literature we treat the general case in which the marginal smooths can be any quadratically penalized basis expansion, and there can be any number of them. We also point out that the imposition of identifiability constraints on smoothers requires more care in the mixed model setting than it would in a simple additive model setting, and show how to deal with the issue. An interesting side effect of our construction is that an ANOVA-decomposition of the smooth can be read off from the estimates, although this is not our primary focus. We were motivated to undertake this work by applied problems in the analysis of abundance survey data, and two examples of this are presented.},
	language = {en},
	number = {3},
	urldate = {2016-03-21},
	journal = {Statistics and Computing},
	author = {Wood, Simon N. and Scheipl, Fabian and Faraway, Julian J.},
	month = feb,
	year = {2012},
	keywords = {functional mixed effect models, generalized additive modeling, interactions, mixed models, multivariate statistics, project: hierarchical additive model tutorial paper, read, regression, tensor methods},
	pages = {341--360},
	file = {Wood_2012_Straightforward intermediate rank tensor product smoothing in mixed models.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2012_Straightforward intermediate rank tensor product smoothing in mixed models.pdf:application/pdf}
}

@phdthesis{marra_problems_2010,
	address = {Bath},
	type = {Doctoral thesis},
	title = {Some problems in model specification and inference for generalized additive models},
	abstract = {Regression models describing the dependence between a univariate response and a set of covariates play a fundamental role in statistics. In the last two decades, a tremendous effort has been made in developing flexible regression techniques such as generalized additive models (GAMs) with the aim of modellingthe expected value of a response variable as a sum of smooth unspecified functions of predictors. Many nonparametric regression methodologies exist including local-weighted regression and smoothing splines. Here the focus is on penalized regression spline methods which can be viewed as a generalization of smoothing splines with a more flexible choice of bases and penalties. This thesis addresses three issues. First, the problem of model misspecifi-cation is treated by extending the instrumental variable approach to the GAM context. Second, we study the theoretical and empirical properties of the con-fidence intervals for the smooth component functions of a GAM. Third, we consider the problem of variable selection within this flexible class of models.
All results are supported by theoretical arguments and extensive simulation experiments which shed light on the practical performance of the methods discussed in this thesis.},
	school = {University of Bath},
	author = {Marra, Giampiero},
	year = {2010},
	keywords = {bayesian statistics, confidence interval estimation, frequentist/Bayesian synthesis, frequentist statistics, generalized additive modeling, parameter estimation, penalization, project: hierarchical additive model tutorial paper, skimmed, smoothing},
	file = {marra_2010_some problems in model specification and inference for generalized additive.pdf:D\:\\Documents\\papers\\indexed_papers\\marra_2010_some problems in model specification and inference for generalized additive.pdf:application/pdf}
}

@article{handcock_kriging_1994,
	title = {Kriging and splines: an empirical comparison of their predictive performance in some applications: comment},
	volume = {89},
	issn = {0162-1459},
	shorttitle = {Kriging and {Splines}},
	url = {http://www.jstor.org.ezproxy.library.wisc.edu/stable/2290838},
	doi = {10.2307/2290838},
	number = {426},
	urldate = {2016-03-10},
	journal = {Journal of the American Statistical Association},
	author = {Handcock, Mark S. and Meier, Kristen and Nychka, Douglas},
	year = {1994},
	keywords = {gaussian processes, kriging, prediction, project: hierarchical additive model tutorial paper, skimmed, smoothing, spatial statistics, splines, statistics for dependence},
	pages = {401--403},
	file = {Handcock_1994_Kriging and Splines.pdf:D\:\\Documents\\papers\\indexed_papers\\Handcock_1994_Kriging and Splines.pdf:application/pdf}
}

@article{vanhatalo_bayesian_2012,
	title = {Bayesian modeling with {Gaussian} processes using the {GPstuff} toolbox},
	url = {http://arxiv.org/abs/1206.5754},
	abstract = {Gaussian processes (GP) are powerful tools for probabilistic modeling purposes. They can be used to define prior distributions over latent functions in hierarchical Bayesian models. The prior over functions is defined implicitly by the mean and covariance function, which determine the smoothness and variability of the function. The inference can then be conducted directly in the function space by evaluating or approximating the posterior process. Despite their attractive theoretical properties GPs provide practical challenges in their implementation. GPstuff is a versatile collection of computational tools for GP models compatible with Linux and Windows MATLAB and Octave. It includes, among others, various inference methods, sparse approximations and tools for model assessment. In this work, we review these tools and demonstrate the use of GPstuff in several models.},
	urldate = {2015-05-04},
	journal = {arXiv:1206.5754 [cs, stat]},
	author = {Vanhatalo, Jarno and Riihim{\"a}ki, Jaakko and Hartikainen, Jouni and Jyl{\"a}nki, Pasi and Tolvanen, Ville and Vehtari, Aki},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.5754},
	keywords = {gaussian processes, Matlab, nonparametric estimation, project: hierarchical additive model tutorial paper, read, software package, statistics for dependence, stochastic processes, time series and statistics for dynamic systems, user manual},
	file = {Vanhatalo_2012_Bayesian modeling with Gaussian processes using the GPstuff toolbox.pdf:D\:\\Documents\\papers\\indexed_papers\\Vanhatalo_2012_Bayesian modeling with Gaussian processes using the GPstuff toolbox.pdf:application/pdf}
}

@incollection{durlauf_analysis_2008,
	address = {Basingstoke},
	edition = {2},
	title = {Analysis of variance},
	isbn = {978-0-333-78676-5},
	url = {http://www.dictionaryofeconomics.com/article?id=pde2008_A000098},
	urldate = {2011-09-08},
	booktitle = {The {New} {Palgrave} {Dictionary} of {Economics}},
	publisher = {Nature Publishing Group},
	author = {Gelman, Andrew},
	editor = {Durlauf, Steven N. and Blume, Lawrence E.},
	year = {2008},
	keywords = {analysis of variance, model selection, multilevel modeling, project: hierarchical additive model tutorial paper, read, statistics},
	pages = {593--598},
	file = {Gelman_2008_Analysis of variance.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2008_Analysis of variance.pdf:application/pdf}
}

@article{thorson_spatial_2015,
	title = {Spatial factor analysis: {A} new tool for estimating joint species distributions and correlations in species range},
	copyright = {This article is protected by copyright. All rights reserved.},
	issn = {2041-210X},
	shorttitle = {Spatial factor analysis},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12359/abstract},
	doi = {10.1111/2041-210X.12359},
	abstract = {Predicting and explaining the distribution and density of species is one of the oldest concerns in ecology. Species distributions can be estimated using geostatistical methods, which estimate a latent spatial variable explaining observed variation in densities, but geostatistical methods may be imprecise for species with low densities or few observations. Additionally, simple geostatistical methods fail to account for correlations in distribution among species, and generally estimate such cross-correlations as a post-hoc exercise. We therefore present spatial factor analysis (SFA), a spatial model for estimating a low-rank approximation to multivariate data, and use it to jointly estimate the joint distribution of multiple species simultaneously. We also derive an analytic estimate of cross-correlations among species from SFA parameters. As a first example, we show that distributions for 10 bird species in the breeding bird survey in 2013 can be parsimoniously represented using only 5 spatial factors. As a second case study, we show that forward-prediction of catches for 20 rockfishes (Sebastes spp.) off the U.S. West Coast is more accurate using spatial factor analysis than analyzing each species individually. Finally, we show that single-species models give a different picture of cross-correlations than joint estimation using SFA. SFA complements a growing list of tools for jointly modelling the distribution of multiple species, and provides a parsimonious summary of cross-correlation without requiring explicit declaration of habitat variables. We conclude by proposing future research that would model species cross-correlations using dissimilarity of species{\textquoteright} traits, and the development of spatial dynamic factor analysis for a low-rank approximation to spatial time-series data. This article is protected by copyright. All rights reserved.},
	language = {en},
	urldate = {2015-02-25},
	journal = {Methods in Ecology and Evolution},
	author = {Thorson, James T. and Scheuerell, Mark D. and Shelton, Andrew O. and See, Kevin E. and Skaug, Hans J. and Kristensen, Kasper},
	month = feb,
	year = {2015},
	keywords = {gaussian processes, movement ecology, multivariate statistics, project: hierarchical additive model tutorial paper, random matrices, random processes, read, spatial ecology, spatial statistics, species distribution model, statistics, statistics for clustering, statistics for dependence},
	pages = {n/a--n/a},
	file = {Thorson_2015_Spatial factor analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Thorson_2015_Spatial factor analysis.pdf:application/pdf}
}

@article{wood_gams_2002,
	title = {{GAMs} with integrated model selection using penalized regression splines and applications to environmental modelling},
	volume = {157},
	abstract = {Generalized Additive Models (GAMs) have been popularized by the work of Hastie
and Tibshirani (1990) and the availability of user friendly GAM software in Splus.
However, whilst it is flexible and efficient, the GAM framework based on backfitting
with linear smoothers presents some difficulties when it comes to model selection and
inference. On the other hand, the mathematically elegant work ofWahba (1990) and
co-workers on Generalized Spline Smoothing (GSS) provides a rigorous framework
for model selection (Gu and Wahba, 1991) and inference with GAMs constructed
from smoothing splines: but unfortunately these models are computationally very
expensive with operations counts that are of cubic order in the number of data.
A {\textquotedblleft}middle way{\textquotedblright} between these approaches is to construct GAMs using penalized
regression splines (see e.g. Wahba 1980, 1990; Eilers and Marx 1998, Wood 2000).
In this paper we develop this idea and show how GAMs constructed using penalized
regression splines can be used to get most of the practical benefits of GSS models,
including well founded model selection and multi-dimensional smooth terms, with
the ease of use and low computational cost of backfit GAMs. Inference with the
resulting methods also requires slightly fewer approximations than are employed
in the GAM modelling software provided in Splus. This paper presents the basic
mathematical and numerical approach to GAMs implemented in the R package
mgcv, and includes two environmental examples using the methods as implemented
in the package.},
	number = {2},
	journal = {Ecological Modelling},
	author = {Wood, S. N and Augustin, N. H},
	year = {2002},
	keywords = {cross-validation, generalized additive modeling, project: forecasting marine species dynamics, project: hierarchical additive model tutorial paper, read, spline regression, splines, statistics},
	pages = {157--177},
	file = {Wood_2002_GAMs with integrated model selection using penalized regression splines and.pdf:D\:\\Documents\\papers\\indexed_papers\\Wood_2002_GAMs with integrated model selection using penalized regression splines and.pdf:application/pdf}
}

@article{westveld_mixed_2011,
	title = {A mixed effects model for longitudinal relational and network data, with applications to international trade and conflict},
	volume = {5},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1310562208},
	doi = {10.1214/10-AOAS403},
	abstract = {The focus of this paper is an approach to the modeling of longitudinal social network or relational data. Such data arise from measurements on pairs of objects or actors made at regular temporal intervals, resulting in a social network for each point in time. In this article we represent the network and temporal dependencies with a random effects model, resulting in a stochastic process defined by a set of stationary covariance matrices. Our approach builds upon the social relations models of Warner, Kenny and Stoto [Journal of Personality and Social Psychology 37 (1979) 1742{\textendash}1757] and Gill and Swartz [Canad. J. Statist. 29 (2001) 321{\textendash}331] and allows for an intra- and inter-temporal representation of network structures. We apply the methodology to two longitudinal data sets: international trade (continuous response) and militarized interstate disputes (binary response).},
	language = {EN},
	number = {2},
	urldate = {2012-01-24},
	journal = {The Annals of Applied Statistics},
	author = {Westveld, Anton H.},
	month = jun,
	year = {2011},
	keywords = {dynamic network, dynamics on graphs, functional mixed effect models, indexed, markov chain Monte Carlo, mixed models, network statistics, network topology, project: hierarchical additive model tutorial paper, Project: network dynamics review, read, statistics, statistics for dependence, statistics for high dimensional systems},
	pages = {843--872},
	file = {Westveld_2011_A mixed effects model for longitudinal relational and network data, with.pdf:D\:\\Documents\\papers\\indexed_papers\\Westveld_2011_A mixed effects model for longitudinal relational and network data, with.pdf:application/pdf}
}

@article{vu_continuous-time_????,
	title = {Continuous-time regression models for longitudinal networks},
	journal = {pre-print},
	author = {Vu, D.Q. and Asuncion, A.U. and Hunter, D.R. and Smyth, P.},
	keywords = {dynamic network, indexed, network, network statistics, network topology, project: hierarchical additive model tutorial paper, Project: network dynamics review, read, statistics, statistics for dependence, statistics for high dimensional systems, survival analysis, time-dependent covariates, time series and statistics for dynamic systems},
	file = {Vu__Continuous-time regression models for longitudinal networks.pdf:D\:\\Documents\\papers\\indexed_papers\\Vu__Continuous-time regression models for longitudinal networks.pdf:application/pdf}
}

@article{kolar_semi-parametric_2010,
	title = {Semi-parametric methods for estimating time-varying graph structure},
	journal = {pre-print},
	author = {Kolar, M.},
	year = {2010},
	keywords = {dynamic network, graph theory, indexed, network, network topology, nonparametric estimation, parameter estimation, project: hierarchical additive model tutorial paper, read, statistics, statistics for dependence, statistics for high dimensional systems, time-varying ecological networks},
	file = {Kolar_2010_Semi-parametric methods for estimating time-varying graph structure.pdf:D\:\\Documents\\papers\\indexed_papers\\Kolar_2010_Semi-parametric methods for estimating time-varying graph structure.pdf:application/pdf}
}

@article{hughes_dimension_2012,
	title = {Dimension reduction and alleviation of confounding for spatial generalized linear mixed models},
	copyright = {{\textcopyright} 2012 Royal Statistical Society},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2012.01041.x/abstract},
	doi = {10.1111/j.1467-9868.2012.01041.x},
	abstract = {Summary. Non-Gaussian spatial data are very common in many disciplines. For instance, count data are common in disease mapping, and binary data are common in ecology. When fitting spatial regressions for such data, one needs to account for dependence to ensure reliable inference for the regression coefficients. The spatial generalized linear mixed model offers a very popular and flexible approach to modelling such data, but this model suffers from two major shortcomings: variance inflation due to spatial confounding and high dimensional spatial random effects that make fully Bayesian inference for such models computationally challenging. We propose a new parameterization of the spatial generalized linear mixed model that alleviates spatial confounding and speeds computation by greatly reducing the dimension of the spatial random effects. We illustrate the application of our approach to simulated binary, count and Gaussian spatial data sets, and to a large infant mortality data set.},
	language = {en},
	urldate = {2012-10-16},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Hughes, John and Haran, Murali},
	year = {2012},
	keywords = {bayesian statistics, functional mixed effect models, generalised linear models, mixed models, project: hierarchical additive model tutorial paper, read, spatial statistics, statistics, statistics for dependence, statistics for high dimensional systems, statistics for networks},
	pages = {no--no},
	file = {Hughes_2012_Dimension reduction and alleviation of confounding for spatial generalized.pdf:D\:\\Documents\\papers\\indexed_papers\\Hughes_2012_Dimension reduction and alleviation of confounding for spatial generalized.pdf:application/pdf;Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\7TX9GERT\\abstract.html:text/html}
}

@article{gelman_induction_2011,
	title = {Induction and deduction in {Bayesian} data analysis},
	volume = {in press},
	abstract = {The classical or frequentist approach to statistics (in which inference is 
centered on significance testing), is associated with a philosophy in which science is 
deductive and follows Popper{\textquoteright}s doctrine of falsification.  In contrast, Bayesian inference 
is commonly associated with inductive reasoning and the idea that a model can be 
dethroned by a competing model but can never be directly falsified by a significance test.
The purpose of this article is to break these associations, which I think are incorrect and 
have been detrimental to statistical practice, in that they have steered falsificationists 
away from the very useful tools of Bayesian inference and have discouraged Bayesians 
from checking the fit of their models.  From my experience using and developing 
Bayesian methods in social and environmental science, I have found model checking and 
falsification to be central in the modeling process.},
	journal = {Rationality, Markets and Morals},
	author = {Gelman, Andrew},
	year = {2011},
	keywords = {bayesian statistics, error-statistical inference, indexed, misidentification, misspecification, philosophy of science, project: hierarchical additive model tutorial paper, read, statistics},
	file = {Gelman_2011_Induction and deduction in Bayesian data analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2011_Induction and deduction in Bayesian data analysis.pdf:application/pdf}
}

@article{gelman_analysis_2005,
	title = {Analysis of variance: {Why} it is more important than ever},
	volume = {33},
	issn = {0090-5364},
	shorttitle = {Analysis of variance?},
	url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1112967698/},
	doi = {10.1214/009053604000001048},
	urldate = {2011-09-08},
	journal = {The Annals of Statistics},
	author = {Gelman, Andrew},
	month = feb,
	year = {2005},
	keywords = {analysis of variance, multilevel modeling, parameter estimation, project: hierarchical additive model tutorial paper, read, statistics},
	pages = {1--53},
	file = {Gelman_2005_Analysis of variance.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2005_Analysis of variance.pdf:application/pdf}
}

@article{gelman_philosophy_2010,
	title = {Philosophy and the practice of {Bayesian} statistics},
	volume = {preprint},
	url = {http://arxiv.org/abs/1006.3868},
	abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
	urldate = {2010-06-28},
	journal = {Arxiv},
	author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
	month = jun,
	year = {2010},
	keywords = {bayesian statistics, frequentist statistics, important reference, philosophy of science, project: hierarchical additive model tutorial paper, read, statistics},
	file = {arXiv.org Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\38U4NCGS\\1006.html:text/html}
}

@article{gelman_bayesian_2006,
	title = {Bayesian measures of explained variance and pooling in multilevel (hierarchical) models},
	volume = {48},
	issn = {0040-1706},
	url = {http://pubs.amstat.org/doi/abs/10.1198/004017005000000517},
	doi = {10.1198/004017005000000517},
	abstract = {Explained variance (R2) is a familiar summary of the fit of a linear regression and has been generalized in various ways to multilevel (hierarchical) models. The multilevel models that we consider in this article are characterized by hierarchical data structures in which individuals are grouped into units (which themselves might be further grouped into larger units), and variables are measured on individuals and each grouping unit. The models are based on regression relationships at different levels, with the first level corresponding to the individual data and subsequent levels corresponding to between-group regressions of individual predictor effects on grouping unit variables. We present an approach to defining R2 at each level of the multilevel model, rather than attempting to create a single summary measure of fit. Our method is based on comparing variances in a single fitted model rather than with a null model. In simple regression, our measure generalizes the classical adjusted R2. We also discuss a related variance comparison to summarize the degree to which estimates at each level of the model are pooled together based on the level-specific regression relationship, rather than estimated separately. This pooling factor is related to the concept of shrinkage in simple hierarchical models. We illustrate the methods on a dataset of radon in houses within counties using a series of models ranging from a simple linear regression model to a multilevel varying-intercept, varying-slope model.},
	number = {2},
	urldate = {2011-06-18},
	journal = {Technometrics},
	author = {Gelman, Andrew and Pardoe, Iain},
	month = may,
	year = {2006},
	keywords = {bayesian statistics, goodness-of-fit testing, indexed, multilevel modeling, project: hierarchical additive model tutorial paper, read},
	pages = {241--251},
	file = {American Statistical Association - Technometrics - 48(2)\:241:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\A9DD7HBA\\004017005000000517.html:text/html;Gelman_2006_Bayesian measures of explained variance and pooling in multilevel (hierarchical).pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2006_Bayesian measures of explained variance and pooling in multilevel (hierarchical).pdf:application/pdf}
}

@article{dray_community_2012,
	title = {Community ecology in the age of multivariate multiscale spatial analysis},
	issn = {0012-9615},
	url = {http://www.esajournals.org/doi/abs/10.1890/11-1183.1},
	doi = {10.1890/11-1183.1},
	journal = {Ecological Monographs},
	author = {Dray, St{\'e}phane and P{\'e}lissier, Rapha{\"e}l and Couteron, Pierre and Fortin, Marie-Jos{\'e}e and Legendre, Pierre and Peres-Neto, Pedro R. and Bellier, Edwige and Bivand, Roger and Blanchet, F. Guillaume and De Caceres, Miquel and Dufour, Anne-B{\'e}atrice and Heegaard, Einar and Jombart, Thibaut and Munoz, Fran{\c c}ois and Oksanen, Jari and Thioulouse, Jean and Wagner, Helene H.},
	year = {2012},
	keywords = {biodiversity, biodiversity statistics, community ecology, community structure, environmental variability, indexed, multivariate statistics, project: hierarchical additive model tutorial paper, read, spatial correlation, spatial sampling, Spatial scale, spatial statistics, statistics, statistics for dependence, statistics for high dimensional systems},
	file = {Dray_2012_Community ecology in the age of multivariate multiscale spatial analysis.pdf:D\:\\Documents\\papers\\indexed_papers\\Dray_2012_Community ecology in the age of multivariate multiscale spatial analysis.pdf:application/pdf;ESA Snapshot:C\:\\Users\\Eric\\AppData\\Roaming\\Zotero\\Zotero\\Profiles\\l6x18nox.default\\zotero\\storage\\M6NCVAPK\\Dray et al. - 2012 - Community ecology in the age of multivariate multi.html:text/html}
}

@article{clough_generalized_2012,
	title = {A generalized approach to modeling and estimating indirect effects in ecology},
	issn = {0012-9658},
	url = {http://www.esajournals.org/doi/abs/10.1890/11-1899.1},
	doi = {10.1890/11-1899.1},
	journal = {Ecology},
	author = {Clough, Yann},
	year = {2012},
	keywords = {bayesian statistics, causality, DAG, D separation, indexed, multilevel modeling, project: hierarchical additive model tutorial paper, read, statistics, statistics for dependence, structural equation modeling},
	file = {Clough_2012_A generalized approach to modeling and estimating indirect effects in ecology.pdf:D\:\\Documents\\papers\\indexed_papers\\Clough_2012_A generalized approach to modeling and estimating indirect effects in ecology.pdf:application/pdf}
}

@article{chung_avoiding_2011,
	title = {Avoiding boundary estimates in linear mixed models through weakly informative priors},
	abstract = {Variance parameters in mixed or multilevel models can be difficult to estimate, especially when the number of groups is small. We propose a maximum penalized likelihood approach which is equivalent to estimating variance parameters by their marginal posterior mode, given a weakly informative prior distribution. By choosing the prior from the gamma family with at least 1 degree of freedom, we ensure that the prior density is zero at the boundary and thus the marginal posterior mode of the group-level variance will be positive. The use of a weakly informative prior allows us to stabilize our estimates while remaining faithful to the data.},
	journal = {working paper},
	author = {Chung, Y. and Rabe-Hesketh, S. and Gelman, A. and Liu, J. and Dorie, V.},
	year = {2011},
	keywords = {bayesian statistics, indexed, multilevel modeling, parameter estimation, project: hierarchical additive model tutorial paper, read, statistics},
	pages = {284},
	file = {Chung_2011_Avoiding boundary estimates in linear mixed models through weakly informative.pdf:D\:\\Documents\\papers\\indexed_papers\\Chung_2011_Avoiding boundary estimates in linear mixed models through weakly informative.pdf:application/pdf}
}

@article{gelman_weakly_2008,
	title = {A weakly informative default prior distribution for logistic and other regression models},
	volume = {2},
	issn = {1932-6157},
	url = {http://projecteuclid.org/euclid.aoas/1231424214},
	doi = {10.1214/08-AOAS191},
	abstract = {We propose a new prior distribution for classical (nonhierarchical) logistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors.},
	language = {EN},
	number = {4},
	urldate = {2011-06-07},
	journal = {The Annals of Applied Statistics},
	author = {Gelman, Andrew},
	month = dec,
	year = {2008},
	note = {Zentralblatt MATH identifier: 1156.62017; Mathematical Reviews number (MathSciNet): MR2655663},
	keywords = {Cauchy distribution, logistic equation, project: hierarchical additive model tutorial paper, regression, weakly informative priors},
	pages = {1360--1383},
	file = {Gelman_2008_A weakly informative default prior distribution for logistic and other regressio.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2008_A weakly informative default prior distribution for logistic and other regressio.pdf:application/pdf}
}

@article{gelman_type_2000,
	title = {Type {S} error rates for classical and {Bayesian} single and multiple comparison procedures},
	volume = {15},
	number = {3},
	journal = {Computational Statistics},
	author = {Gelman, A. and Tuerlinckx, F.},
	year = {2000},
	keywords = {project: hierarchical additive model tutorial paper},
	pages = {373--390},
	file = {Gelman_2000_Type S error rates for classical and Bayesian single and multiple comparison.pdf:D\:\\Documents\\papers\\indexed_papers\\Gelman_2000_Type S error rates for classical and Bayesian single and multiple comparison.pdf:application/pdf}
}